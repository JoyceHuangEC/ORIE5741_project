{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import functools\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import random as rand\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the data. We will also print a list of each columns together with its datatype, using the \"type\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(26)\n",
    "df = pd.read_csv('tmdb_5000_movies.csv') # Load in the csv file \n",
    "print(df.shape)\n",
    "df = df.loc[~((df['budget'] == 0) | (df['revenue'] == 0))]\n",
    "print(df.shape)\n",
    "\n",
    "feature_names = df.columns\n",
    "for i in range(len(feature_names)):\n",
    "    print(str(i), \"\\t\", str(feature_names[i]),\"\\t\\t\\t\", str(type(df.iloc[0,i])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dealing with Nan/empty data\n",
    "\n",
    "Our goal is to predict the success of a movie (represented by its revenue) provided a list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete id column (which is not relevent to our task)\n",
    "df = df.drop([\"id\"], axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "feature_names = df.columns\n",
    "\n",
    "# delete rows which contains empty field\n",
    "for feature in feature_names:\n",
    "    if (feature != \"homepage\" and feature != \"tagline\"):\n",
    "        df[feature].replace('', np.nan, inplace=True)\n",
    "        df = df[~df[feature].isna()]\n",
    "    else:\n",
    "        df[feature].replace(np.nan, \"\", inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Train/Test\n",
    "\n",
    "optional: k-fold etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1) # we shuffle the data so that our train/test split will be truly random\n",
    "\n",
    "train_proportion = 0.8\n",
    "n = len(df)\n",
    "print('Size of dataset: ', str(n))\n",
    "\n",
    "# Put the first ntrain observations in the DataFrame df into the training set, and the rest into the test set\n",
    "t = int(train_proportion * n)\n",
    "\n",
    "def regression_to_classification(i):\n",
    "    return int(math.log10(abs(i)))\n",
    "\n",
    "target = df['revenue']\n",
    "labels = df.loc[:, df.columns.isin(['revenue'])].applymap(regression_to_classification)\n",
    "data = df.loc[:, ~df.columns.isin(['revenue'])]\n",
    "\n",
    "# the following variable records the features of examples in the training set\n",
    "train_x = data.iloc[0:t]\n",
    "# the following variable records the features of examples in the test set\n",
    "test_x = data.iloc[t:]\n",
    "# the following variable records the labels of examples in the training set\n",
    "train_y = target[0:t]\n",
    "# the following variable records the labels of examples in the test set\n",
    "test_y = target[t:]\n",
    "# the following variable records the label of examples in the training set\n",
    "train_label = labels[0:t]\n",
    "# the following variable records the label of examples in the test set\n",
    "test_label = labels[t:]\n",
    "\n",
    "# let's take a look\n",
    "print('Training dataset: ', train_x)\n",
    "print('Training y: ',train_y)\n",
    "print('Training label: ',train_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boolean and Real Value Data\n",
    "\n",
    "homepage: whether a movie has a home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change homepage to boolean feature: whether a movie has a home page\n",
    "def string_to_bool(string):\n",
    "    if (string != \"\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# change tagline to int feature: length of the tagline (if no tagline, len = 0)\n",
    "def string_to_int(string):\n",
    "    try:\n",
    "        return len(string.split())\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_x['vote_count'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_str_to_bool = ['homepage']\n",
    "label_str_to_int = ['tagline']\n",
    "label_str_to_real = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
    "\n",
    "train_vals_homepage = np.asarray(train_x.loc[:, train_x.columns.isin(label_str_to_bool)].applymap(string_to_bool))\n",
    "test_vals_homepage = np.asarray(test_x.loc[:, test_x.columns.isin(label_str_to_bool)].applymap(string_to_bool))\n",
    "\n",
    "train_vals_tagline = np.asarray(train_x.loc[:, train_x.columns.isin(label_str_to_int)].applymap(string_to_int))\n",
    "test_vals_tagline = np.asarray(test_x.loc[:, test_x.columns.isin(label_str_to_int)].applymap(string_to_int))\n",
    "\n",
    "train_vals_real = np.asarray(train_x.loc[:, train_x.columns.isin(label_str_to_real)])\n",
    "test_vals_real = np.asarray(test_x.loc[:, test_x.columns.isin(label_str_to_real)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Onehot Data\n",
    "\n",
    "features: production companyï¼Œcountry, spoken language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(string):\n",
    "    data = json.loads(string)\n",
    "    result = {d['id']: d['name'] for d in data}\n",
    "    return result.values()\n",
    "\n",
    "label_str_onehot = [\n",
    "    'original_language', \n",
    "]\n",
    "\n",
    "# print(train_x.columns)\n",
    "\n",
    "#Sets of all categories in a particular column\n",
    "label_onehot_set = [train_x.loc[:, label].fillna('NaN').unique() for label in label_str_onehot]\n",
    "print(label_onehot_set[0])\n",
    "\n",
    "def onehot(column=None, col=None):\n",
    "    # print(col)\n",
    "    result = []\n",
    "    for data in column:\n",
    "        # print(data)\n",
    "        dic = dict.fromkeys(list(col), 0)\n",
    "        # print(dic)\n",
    "        if data in dic.keys():\n",
    "            dic[data] += 1\n",
    "        result.append(list(dic.values()))\n",
    "    return result\n",
    "\n",
    "train_vals_onehot = train_x.loc[:, train_x.columns.isin(label_str_onehot)]\n",
    "test_vals_onehot = test_x.loc[:, test_x.columns.isin(label_str_onehot)]\n",
    "\n",
    "def process_onehot():\n",
    "    trains = np.ones((len(train_vals_onehot),1))\n",
    "    tests = np.ones((len(test_vals_onehot),1))\n",
    "    for i in range(len(label_str_onehot)):\n",
    "        train_data_col = train_vals_onehot[label_str_onehot[-i]]\n",
    "        # print(label_str_onehot[-i])\n",
    "        test_data_col = test_vals_onehot[label_str_onehot[-i]]\n",
    "        feature_list = label_onehot_set[-i]\n",
    "        train_vector = onehot(train_data_col, feature_list)\n",
    "        # print(feature_list)\n",
    "        test_vector = onehot(test_data_col, feature_list)\n",
    "        trains = np.concatenate((train_vector, trains), 1)\n",
    "        tests = np.concatenate((test_vector, tests), 1)\n",
    "    # print(trains[:, :-1].shape)\n",
    "    # print(trains[:, :-1])\n",
    "    return trains[:, :-1], tests[:, :-1]\n",
    "\n",
    "train_vals_onehot, test_vals_onehot = process_onehot()\n",
    "print(train_vals_onehot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Manyhot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(string):\n",
    "    data = json.loads(string)\n",
    "    result = {d['id']: d['name'] for d in data}\n",
    "    return result.values()\n",
    "\n",
    "def string_to_list_prod_countries(string):\n",
    "    data = json.loads(string)\n",
    "    result = {d['iso_3166_1']: d['iso_3166_1'] for d in data}\n",
    "    return result.values()\n",
    "\n",
    "def string_to_list_spoken_lang(string):\n",
    "    data = json.loads(string)\n",
    "    result = {d['iso_639_1']: d['iso_639_1'] for d in data}\n",
    "    return result.values()\n",
    "\n",
    "def manyhot(column=None, col=None):\n",
    "    cat_to_idx = {cat: i for i, cat in enumerate(col)}\n",
    "    manyhot_vectors = []\n",
    "    for entry in column:\n",
    "        vec = [0] * len(col)\n",
    "        for ele in entry:\n",
    "            if (ele in cat_to_idx):\n",
    "                vec[cat_to_idx[ele]] = 1\n",
    "        manyhot_vectors.append(vec)\n",
    "    return np.asarray(manyhot_vectors)\n",
    "\n",
    "label_str_to_dict = ['genres', 'keywords', 'production_companies']\n",
    "train_vals_genres_keywords = train_x.loc[:, train_x.columns.isin(label_str_to_dict)].applymap(string_to_list)\n",
    "test_vals_genres_keywords = test_x.loc[:, test_x.columns.isin(label_str_to_dict)].applymap(string_to_list)\n",
    "\n",
    "# set of all genres\n",
    "all_genres = functools.reduce(lambda x, y: x.union(y), train_vals_genres_keywords['genres'], set())\n",
    "train_vals_genres = manyhot(column=train_vals_genres_keywords['genres'], col=all_genres)\n",
    "test_vals_genres = manyhot(column=test_vals_genres_keywords['genres'], col=all_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list using a list comprehension\n",
    "prod_company_list = [company for sublist in train_vals_genres_keywords['production_companies'] for company in sublist]\n",
    "prod_company_freq = Counter(prod_company_list)\n",
    "# Create a list of companies to remove\n",
    "prod_company_to_remove = [company for company, count in prod_company_freq.items() if count <= 3]\n",
    "# Remove the keys from the dictionary\n",
    "for company in prod_company_to_remove:\n",
    "    del prod_company_freq[company]\n",
    "all_prod_companies = prod_company_freq.keys()\n",
    "train_vals_prod_company = manyhot(column=train_vals_genres_keywords['production_companies'], col=all_prod_companies)\n",
    "test_vals_prod_company = manyhot(column=test_vals_genres_keywords['production_companies'], col=all_prod_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list using a list comprehension\n",
    "flat_keywords_list = [keyword for sublist in train_vals_genres_keywords['keywords'] for keyword in sublist]\n",
    "keywords_freq = Counter(flat_keywords_list)\n",
    "# Create a list of keys to remove\n",
    "keys_to_remove = [key for key, count in keywords_freq.items() if count <= 3]\n",
    "# Remove the keys from the dictionary\n",
    "for key in keys_to_remove:\n",
    "    del keywords_freq[key]\n",
    "all_keywords = keywords_freq.keys()\n",
    "train_vals_keywords = manyhot(column=train_vals_genres_keywords['keywords'], col=all_keywords)\n",
    "test_vals_keywords = manyhot(column=test_vals_genres_keywords['keywords'], col=all_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature of spoken language\n",
    "train_spoken_lang_keywords = train_x.loc[:, train_x.columns.isin(['spoken_languages'])].applymap(string_to_list_spoken_lang)\n",
    "test_spoken_lang_keywords = test_x.loc[:, test_x.columns.isin(['spoken_languages'])].applymap(string_to_list_spoken_lang)\n",
    "# set of all spoken lang\n",
    "all_lang = functools.reduce(lambda x, y: x.union(y), train_spoken_lang_keywords['spoken_languages'], set())\n",
    "train_vals_lang = manyhot(column=train_spoken_lang_keywords['spoken_languages'], col=all_lang)\n",
    "test_vals_lang = manyhot(column=test_spoken_lang_keywords['spoken_languages'], col=all_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature of production_countries\n",
    "train_prod_country_keywords = train_x.loc[:, train_x.columns.isin(['production_countries'])].applymap(string_to_list_prod_countries)\n",
    "test_prod_country_keywords = test_x.loc[:, test_x.columns.isin(['production_countries'])].applymap(string_to_list_prod_countries)\n",
    "# set of all spoken lang\n",
    "all_prod_country = functools.reduce(lambda x, y: x.union(y), train_prod_country_keywords['production_countries'], set())\n",
    "train_vals_prod_country = manyhot(column=train_prod_country_keywords['production_countries'], col=all_prod_country)\n",
    "test_vals_prod_country = manyhot(column=test_prod_country_keywords['production_countries'], col=all_prod_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_str_onehot = [\n",
    "#     'production_companies', \n",
    "#     'production_countries', \n",
    "#     'spoken_languages'\n",
    "# ]\n",
    "# print(train_x['spoken_languages'][0])\n",
    "# print(train_vals_prod_company.shape)\n",
    "# print(train_vals_lang[5])\n",
    "# print(train_vals_prod_country[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_offset = np.ones((train_vals_homepage.shape[0], 1))\n",
    "test_offset = np.ones((test_vals_homepage.shape[0], 1))\n",
    "train_vals = np.concatenate((train_vals_homepage, train_vals_tagline, train_vals_onehot, train_vals_real, train_vals_prod_company, train_vals_prod_country, train_vals_lang, train_vals_genres, train_offset), axis=1)\n",
    "test_vals = np.concatenate((test_vals_homepage, test_vals_tagline, test_vals_onehot, test_vals_real, test_vals_prod_company, test_vals_prod_country, test_vals_lang, test_vals_genres, test_offset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just computes the mean squared error\n",
    "def MSE(y, pred):\n",
    "    return np.mean(np.square(y - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs OLS and bypasses any SVD (Singular Value Decomposition) convergence errors by refitting the model\n",
    "def run_OLS(train_y, test_y, train_vals, test_vals):\n",
    "    ols_model = sm.regression.linear_model.OLS(train_y, train_vals)\n",
    "    while True: # Bypasses SVD convergence assertion error\n",
    "        try:\n",
    "            results = ols_model.fit()\n",
    "            break\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "    w = np.array(results.params).reshape([len(results.params),1])\n",
    "\n",
    "    train_pred = np.matmul(train_vals,w)\n",
    "    test_pred = np.matmul(test_vals,w)\n",
    "\n",
    "    train_MSE = MSE(train_y, train_pred.flatten())\n",
    "    test_MSE = MSE(test_y, test_pred.flatten())\n",
    "    \n",
    "    return train_MSE, test_MSE, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MSE, test_MSE, test_pred = run_OLS(train_y, test_y, train_vals, test_vals)\n",
    "\n",
    "print(\"Train MSE\\t\", str(train_MSE))\n",
    "print(\"Test MSE\\t\", str(test_MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a nonlinear model\n",
    "model_classifier = MLPClassifier(hidden_layer_sizes=(20, 20))\n",
    "model_regressor = MLPRegressor(hidden_layer_sizes=(20, 20))\n",
    "\n",
    "# Train the model on the train set\n",
    "model_classifier.fit(train_vals, train_label)\n",
    "model_regressor.fit(train_vals, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set\n",
    "train_pred = model_classifier.predict(train_vals)\n",
    "test_pred = model_classifier.predict(test_vals)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "accuracy_train = accuracy_score(train_label, train_pred)\n",
    "accuracy_test = accuracy_score(test_label, test_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Train accuracy:\", accuracy_train)\n",
    "print(\"Test accuracy:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set\n",
    "train_pred = model_regressor.predict(train_vals)\n",
    "test_pred = model_regressor.predict(test_vals)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "print(metrics.r2_score(train_y, train_pred))\n",
    "print(metrics.mean_squared_log_error(train_y, train_pred))\n",
    "print(metrics.r2_score(test_y, test_pred))\n",
    "print(metrics.mean_squared_log_error(test_y, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv5741': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "557b5432e63c25bc0a47064c92a1c7acaa548717fa75dcf313c4d9862074aa00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
